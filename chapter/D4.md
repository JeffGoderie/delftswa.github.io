#Metrics
This section describes the quality of the software, using the [GQM](http://www.im-institute.org/gqm/gqm%20guide%20non%20printable.pdf). 
The complete approach is summarised in the following diagram:

![GQM diagram](gqm.png)
<!--- 
	You can adapt the diagram on this page, please update the gqm.png snapshot.
http://www.nomnoml.com/#view/%0A[Goal]%0A[Goal]->[Scalability]%0A[Goal]->[Maintainability]%0A[Goal]->[Adoption]%0A[Scalability]->[Resources%20per%20request]%0A[Scalability]->[Throughput]%0A[Maintainability]->[Module%20size%20LOC/module]%0A[Maintainability]->[Module%20size%20over%20time]%0A[Maintainability]->[#Spin-off%20modules]%0A[Adoption]->[Stackoverflow%20questions]%0A[Adoption]->[Job%20positions]%0A%0A
-->

##Goal
Play is a framework that aims to be chosen by its clients in cases where large web applications have to be developed. 
Therefore, one of the major goal of the developers of Play can be defined as: _"Developing an attractive web application framework that users want to build web applications of scale with."_


Attractive is objective and may be interpreted by a broad set of characteristics such as; reliable, robust, flexible, maintainable, etc. 
The following sections will go in depth on some of these characteristics.

##Questions
In this section, three questions will be formulated that focus on characterising the assessment or achievement of the goal that has been defined in the section above.

###Maintainability
The first metric is defined by the maintainability of the system. 
We define this as _"How maintainable is the Play Framework?"_. 
Maintainability forms an important aspect, the web environment and the applications around it develop over time. 
The technology advances over the past ten years are vast and to keep up with further technological demands, the system must be able to adapt to this.


###Scalability
The second question that can be formulated is _"How scalable is the Play framework?"_.
Scalability is an important aspect for a web development framework that aims to support large web applications. 
It ensures that a web application continues to operate according to standards, while the demand increases.

###Adoption

The third questions relate to the extend to which it useful to invest in learning Play Framework.
It is formulated as _"How is the environment adopted to Play Framework?"_.
For this question we try to measure the programming environment and show to which extend the community is active supporting Play.


##Metrics

###Scalability
There are two types of scalability that can be measured. Horizontal scalability is the amount of nodes that can be added to a system in order to improve the speed under large loads. Vertical scalability is the amount of resources (in processing power) that can be added to a node to improve the speed of the system. Play offeres built in horizontal scaling principles, for example that it is stateless by nature. However, in this question we will focus on vertical scalability, as clearer measurements exists for measuring the characteristics of this type of scalability.
 
The metrics that can be defined for measuring vertical scalability of a web application are:

* Throughput
* Latency

#####Throughput
The throughput of a system is a key aspect that tells a lot about the efficiently of the application. 
The higher the throughput, the more requests it may be able to handle per unit of time.
This increases the amount of requests that can be handled when the demand of requests is high and thus increases the scalability of the framework.

The metric we define for throughput is: _HTTP requests/second (RPS)_.

The HTTP request in Play 2.3 and earlier are by default [handled by Netty](https://www.playframework.com/documentation/1.1/api/play/mvc/class-use/Http.Request.html). 
Several tests have been performed between the most commonly used web development frameworks. 
A few of the results are listed below.

* 9.000 requests/second in a test performed by [TechEmpower](http://www.techempower.com/blog/2013/04/05/frameworks-round-2/). The test consisted of JSON-request using an Amazon EC2 Large instance. Netty itself takes the lead with a staggering 37.9k request/sec.
* 2.400 requests/second in a test performed by [Christian Papauschek](http://blog.papauschek.com/2013/04/real-world-performance-of-the-play-framework-on-ec2/) in 2013. In this test, a Play 2.1.1-scala production instance running on EC2 large was used to service complete webpages.


#####Time per request
Latency is a relevant metric for web applications, because it provides information about the efficiency of handling a request. 
The more efficient a request is handled, the lower the latency in large demands and thus the more scalable the application is.

The metric (applied to Play) we define for this is: _ms/HTTP request_.

The Play framework maintains a testing tool ([Prune](https://github.com/playframework/prune)) that automates the performance tests of different versions of Play framework. 
One of the metrics that is tested is latency. 

Prune automates 6 tests cases that differ in the kind of operation. 
Test cases vary from a simple plain text response of `Hello world` to a download response of 50kbit. 
 
In the test results below it is shown that the latency of the 50k requests is significantly higher (~40ms) than the latency of the plain text response (~0.5ms). 

![](./50k_chuck.png)
_50kbit request_

![](./simple_request.png)
_Simple plain text request_


A typical difference between Scala and Java applications of Play users exists. 
Writing in Scala generally allows for more readable and understandable asynchronous code, versus the traditional synchronous Java style. 
James Roper [describes](http://architects.dzone.com/articles/scaling-scala-vs-java) this as "Scala helps you scale much more than Java does". 
In a typical example case the Java variant would be written in a blocking fashion, degrading the performance of concurrent requests, lowering vertical scalability. 
This effect may contribute to the difference in the latency measurements between a Java and a Scala application.

Judging from these graphs, we can see that the Play Framework performs constantly well on the metrics.
With the exception of a few versions.
We think it is important to keep a close watch on these metrics, as they tell about the performance of the system. 







###Maintainability
It is in the interest of the client and the development team that a framework 
is maintainable. Having maintainable software means that more people can 
contribute, new features can be added and potiential bugs might be fixed 
faster. Each software product essentially has the same lifespan as the 
frameworks it uses. When a framework becomes unmaintainable or deprecated no 
further security fixes or updates will be released, which is a problem for 
your product. It is therefore really important that Play is maintainable in 
order to be a good choice.

Maintainability has many aspects. It is important that the source builds 
quickly, minimal setup is needed, and that there are not too much platform 
dependencies. All of these aspects are hard to measure in exact numbers, and 
to relate this to maintainability in an objective manner. We have ourselves 
build and tested Play and used the Jenkins build system, and are confident 
that these aspects really contribute to the maintainability positively.

Seeking for objective metrics we find Test Coverage. Enough tests must exist 
to guarantee stability after maintenance. This prevents bugs and gives 
developers immediate feedback and a level of confidence that your change does 
not break or introduces bugs elsewhere. For test coverage exact metrics exist. 
The following metric can be described:

#####Test Coverage
**Entity:** The Play Framework code    
<br>**Attribute:** Test Coverage   
<br>**Mapping:** Count the Lines of Code that have been covered by tests using a Test Coverage plugin   
<br>**Measure:** percentage      
<br>**Scale:** ratio

One of the other aspects of maintainability is modularisation. Modularisation 
ensures that functionality is decoupled and concentrated in logical places. 
This helps when maintaining a specific feature, or finding a specific bug. 
Typesafe has shown interest in modularising Play for quite some time and 
continues this process. Therefore we look at modularisation over time in the 
following metrics.

#####Module size (LOC) over time
**Entity:** Play's modules
<br>**Attribute:** Size
<br>**Mapping:** Count the Lines of Code (without comments) using a software tool like CLOC, per module
<br>**Measure:** number      
<br>**Scale:** non-negative integer

We think that smaller modules are preferable and contribute to 
maintainability, as less code needs to be interpreted when working on a 
specific module. Visualising the size of the modules over time shows how new 
features are developed and how they at some time get refactored to be a 
separate module. In the [next section](#Modules-over-time) this metric is 
executed.

#####Number of spin-off modules
**Entity:** The Play Framework code
<br>**Attribute:** Externalisation
<br>**Mapping:** Count the number of modules that are removed from the core and have become projects on their own
<br>**Measure:** number
<br>**Scale:** non-negative integer

Examples of spin-off modules are [Twirl](https://github.com/playframework/twirl) and [Anorm](https://github.com/playframework/anorm). 
Externalisation of modules allows the modules to be used by other developers 
outside of the scope of Play. This improves maintainability too because those 
other developers do not need to bother about the code of Play, and Play does 
not contain the modules code anymore. Basically the footprint of Play and thus 
size of the mental model Play's maintainers need to have decreases.

#### Modules over time
Modularisation being on the roadmap of Play caused us to be interested in 
researching the module size over time. For this all 5000+ commits merged in to 
the master branch where examined and LOC where calculated for all subfolders 
of the `framework/src` folder, as this parent folder contains all modules.

The result of this metric is shown [interactively at our gh-pages](http://delftswa2014.github.io/team-playframework/modules.html). The graph basically looks like this:

![Module Size over time (LOC)](https://cloud.githubusercontent.com/assets/791189/6792064/a2478cea-d1b4-11e4-824d-4d62187bb67b.png).

Several developments can be seen in this graph. The following table summarizes the major changes, that where extracted from this graph:

Date 	| Refactoring | Reference	  
--------|-------------|---------------
Jan 5, 2012 | Anorm refactoring | [playframework/commit/f35fd305](https://github.com/playframework/playframework/commit/f35fd305d98038ab0fe3f8076368ee7f8d90015b)
Oct 10, 2012 | Moved iteratees to a project | [playframework/commit/1305c5f8](https://github.com/playframework/playframework/commit/1305c5f8)
Oct 12, 2012 | Guillaume splitting main project | [playframework/commit/0c90761e](https://github.com/playframework/playframework/commit/0c90761e)
Feb 21, 2013 | Splitting Play JSON / Play Functional and Play Data commons | [playframework/commit/dfa557ac](https://github.com/playframework/playframework/commit/dfa557ac)
Apr 24, 2014 | Externalising Twirl | [playframework/pull/2684](https://github.com/playframework/playframework/pull/2684) + Jane Austin quote
Nov 26, 2014 | SBT-plugin refactoring | [playframework/pull/3667](https://github.com/playframework/playframework/pull/3667)
Dec 8, 2014 | Externalising Anorm | [playframework/pull/3712](https://github.com/playframework/playframework/pull/3712)

This shows that our metric is quite usable for finding large changes which are hard to find just browsing to the git history. 
The graph also shows lots of effort to increase the level of modularisation, increasing maintainability and 'giving something back' to the community in the form of (even more) general purpose libraries.

###Adoption 
In this section we describe the metrics related to the adoption question.

#####Number of job openings
For this metric we look at the number of job openings on [indeed](http://www.indeed.com).
We compare the number of job openings for each framework.
The higher the number, the more adopted the framework is in companies. 

To measure this metric we compare it to a number of other frameworks.
We compare it to two JavaScript frameworks: Node.js and AngularJS, two JVM based frameworks: Scalatra and Spring MVC, and to ASP.NET MVC framework.  

![job opnenings barchart](job.png)

From this bar chart we can judge that the Play Framework has some job openings, but they are relatively small compared to existing frameworks.
Judging from this we can state that Play has not been adopted as much as other major players.
Still, working as a developer pays off, as the salaries range in $90k-$180k a year.

This metric does not say everything and should only be taken only as an indication.
It could be that there is a lot more adoption of Play, but that it is generally not listed on job sites.

Another question that arose while looking at this data was: What language should we program in if we want to become a Play Framework developer?
We looked at the 198 openings and then searched whether they required Java or Scala as a skill.
What we found is summarised in the following pie chart:

![Extra skill pie chart](extraSkill.png)

#####Stackoverflow questions

